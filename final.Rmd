---
title: "US_Accidents_Prediction"
author: "Team: Bhoomika Nanjaraja, Khush Shah, Apoorva Reddy Bagepalli, Devarsh Apurva Sheth"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes some helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
# You will need to install it (once) from GitHub.
# library(devtools)
# devtools::install_github("physicsland/ezids")
# Then load the package in your R session.
library(ezids)
library(ggplot2)
library(caTools)
library(glmnet)
library(caret)
library(pROC)
library(PRROC)
library(dplyr)
library(lubridate)
library(tidyverse)
library(scales)
library(plotly)
library(gridExtra)
library(tidytext)
library(modelr)
library(caret)
library(ROSE)
library(randomForest)
library(ggrepel)
library(patchwork)
library(readr)
library(corrplot)
```

EDA

```{r}
df = read.csv('C:/Users/nupur/computer/Desktop/Intro_DS/DATS6101_Project/US_Accidents.csv')
head(df, 5)
```

```{r}
str(df)
sum(is.na(df))
```

```{r}
# Load the dataset (assuming your dataset is stored in a variable called 'accidents_data')
accidents_data <- read.csv('US_accidents.csv')

# Identify streets with the highest severity proportions
highest_severity_streets <- accidents_data %>%
  arrange(desc(SeverityProportion)) %>%
  slice(1)

# Identify streets with the lowest severity proportions
lowest_severity_streets <- accidents_data %>%
  arrange(SeverityProportion) %>%
  slice(1)

# Display the results
cat("Streets with the Highest Severity Proportions:\n")
print(highest_severity_streets[, c("City", "Street", "SeverityProportion")])

cat("\nStreets with the Lowest Severity Proportions:\n")
print(lowest_severity_streets[, c("City", "Street", "SeverityProportion")])
```

```{r}
accidents_data$Weighted_Response_Time <- accidents_data$Weights

# Alternatively, plot a boxplot to visualize the distribution
ggplot(accidents_data, aes(x = 1, y = Weighted_Response_Time)) +
  geom_boxplot(fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Boxplot of Weighted Response Times",
       x = "",
       y = "Weighted Response Time") +
  coord_cartesian(ylim = c(0, 0.0001))

```

```{r}
dataframe_cleaned <- na.omit(df)
str(dataframe_cleaned)

dataframe_cleaned$City <- as.factor(dataframe_cleaned$City)
names(dataframe_cleaned)[names(dataframe_cleaned) == "New Amenity"] <- "New_Amenity"
dataframe_cleaned$Amenity<- as.factor(dataframe_cleaned$Amenity)
dataframe_cleaned$New_Amenity<- as.factor(dataframe_cleaned$New_Amenity)
#dataframe_cleaned$`Response time`<- as.factor(dataframe_cleaned$`Response time`)
dataframe_cleaned$CityAbr <- substr(dataframe_cleaned$City, start = 1, stop = 3)
```

```{r}
ggplot(dataframe_cleaned, aes(x = CityAbr,fill = New_Amenity)) + geom_bar(position = "dodge")
```

```{r}
ggplot(dataframe_cleaned, aes(x = CityAbr,fill = Amenity)) + geom_bar(position = "dodge")
```

```{r}
category_counts <- table(dataframe_cleaned$Amenity)
barplot(category_counts, 
        main = "Bar Plot of Categories", 
        xlab = "Categories", 
        ylab = "Frequency",
        col = "blue")
```

```{r}
category_counts <- table(dataframe_cleaned$New_Amenity)
barplot(category_counts, 
        main = "Bar Plot of Categories", 
        xlab = "Categories", 
        ylab = "Frequency",
        col = "blue")
```

```{r}
ggplot(dataframe_cleaned, aes(x = `Response time`,fill = Amenity)) + geom_bar(position = "dodge")
```

```{r}
ggplot(dataframe_cleaned, aes(x = `Response time`,fill = New_Amenity)) + geom_bar(position = "dodge")
```

```{r}
dataframe_cleaned$Sevprop_mod <- dataframe_cleaned$SeverityProportion * 1000000
dataframe_cleaned$Weights_mod <- dataframe_cleaned$Weights * 10000
unique_values1 <- unique(dataframe_cleaned$Sevprop_mod)
print(unique_values1)
unique_values2 <- unique(dataframe_cleaned$Weights_mod)
print(unique_values2)
```
```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod)) + 
     geom_boxplot() +
  labs(title = "Boxplot", 
       x = "SeverityProp", 
        y = "Count")+scale_x_continuous(limits = c(1, 10))
```
```{r}
ggplot(dataframe_cleaned, aes(x = Weights_mod)) + 
     geom_boxplot() +
  labs(title = "Boxplot", 
       x = "Weight", 
        y = "Count")+scale_x_continuous(limits = c(1, 10))
```
```{r}
df_sub <- subset(dataframe_cleaned,dataframe_cleaned$Amenity==0)
ggplot(df_sub, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```

```{r}
df_sub <- subset(dataframe_cleaned,dataframe_cleaned$New_Amenity==0)
ggplot(df_sub, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```

```{r}
df_subnew <- subset(dataframe_cleaned,dataframe_cleaned$Amenity==1)
ggplot(df_subnew, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```
```{r}
df_subnew <- subset(dataframe_cleaned,dataframe_cleaned$New_Amenity==1)
ggplot(df_subnew, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```
```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod, y = Weights_mod,color = New_Amenity)) +
    geom_point() +  
    ggtitle("Scatter Plot with Hue") +
    xlab("Sev(Prop)") +
    ylab("Weights") 
```
```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod, y = Weights_mod,color = Amenity)) +
    geom_point() +  
    ggtitle("Scatter Plot with Hue") +
    xlab("Sev(Prop)") +
    ylab("Weights") 
```

```{r}
numeric_data <- dataframe_cleaned[, sapply(dataframe_cleaned, is.numeric)]
cor_matrix <- cor(numeric_data)
corrplot(cor_matrix, method = "number")
```


Modelling
```{r}
set.seed(123)
```

## Train-Test Split Explanation

The provided R code performs a train-test split on a dataframe (`df`). This is a common practice in machine learning to assess the performance of a model on unseen data.
```{r}
indices <- sample(1:nrow(df), size = 0.72 * nrow(df))

# Create train and test datasets
train_data <- df[indices, ]
test_data <- df[-indices, ]
```

```{r}
train_data <- train_data[, !(colnames(train_data) %in% c('City', 'Street'))]
model_formula <- New.Amenity ~ .
model <- glm(model_formula, train_data, family = "binomial")
summary(model)
```
## Logit Equation:

\[
\text{{logit}}(p) = -4.784 \times 10^3 - 4.46 \times 10^8 \times \text{{Severity\_Proportion}} - 5.626 \times 10^8 \times \text{{Amenity}} - 38.94 \times \text{{Response\_time}} + 5.626 \times 10^8 \times \text{{Weights}}
\]


## Interpretation of Logistic Regression Coefficients

- **Intercept (-4.784e+03):** The intercept represents the log-odds of the response variable being 1 when all predictor variables are zero. A large negative value suggests a low probability of the response variable being 1 in the absence of other factors.

- **Severity_Proportion (-4.46e+08):** For every one-unit increase in Severity_Proportion, the log-odds of the response variable being 1 decrease by \(4.46 \times 10^8\). The negative sign suggests that higher Severity_Proportion is associated with a lower probability of the event.

- **Amenity (-5.626e+08):** Similarly, for every one-unit increase in Amenity, the log-odds of the response variable being 1 decrease by \(5.626 \times 10^8\). The negative sign suggests that higher Amenity is associated with a lower probability of the event.

- **Response_time (-38.94):** For every one-unit increase in Response_time, the log-odds of the response variable being 1 decrease by 38.94. The negative sign suggests that a longer Response_time is associated with a lower probability of the event.

- **Weights (5.626e+08):** For every one-unit increase in Weights, the log-odds of the response variable being 1 increase by \(5.626 \times 10^8\). The positive sign suggests that higher Weights are associated with a higher probability of the event.

**Overall Interpretation:**
- The significant coefficients indicate that Severity_Proportion, Amenity, Response_time, and Weights are statistically associated with the response variable.
- Higher values of Severity_Proportion and Amenity are associated with a lower probability of the event.
- Longer Response_time is associated with a lower probability of the event.
- Higher Weights are associated with a higher probability of the event.

```{r}
test_data <- test_data[, !(colnames(test_data) %in% c('City', 'Street'))]
head(test_data,5)
```

```{r}
predicted_probs <- predict(model, newdata = test_data, type = "response")
predicted_labels <- ifelse(predicted_probs > 0.0001, 1, 0)

# Confusion Matrix and Accuracy
conf_matrix <- table(test_data$New.Amenity, predicted_labels)
accuracy <- sum(diag(conf_matrix))/sum(conf_matrix)
print(paste("Accuracy:", accuracy))
```

- **Predicting Probabilities:**
The predict function is used to obtain predicted probabilities from the trained model (model) on the test dataset (test_data).

- **Converting Probabilities to Binary Labels:**
The predicted probabilities are then converted into binary labels using a threshold of 0.0001. If the predicted probability is greater than 0.0001, the label is set to 1; otherwise, it is set to 0.

- **Creating Confusion Matrix:**
A confusion matrix (conf_matrix) is created by comparing the predicted binary labels with the true labels from the test dataset (test_data$New_Amenity).

- **Calculating Accuracy:**
The accuracy of the model is calculated by summing the diagonal elements (correct predictions) of the confusion matrix and dividing it by the total number of predictions.

The accuracy is calculated as the sum of correct predictions (diagonal elements of the confusion matrix) divided by the total number of predictions. If the accuracy is printed as "1," it suggests that all predictions made by the model on the test dataset were correct. This could be due to a few reasons:

**Perfect Model:** The model might be performing exceptionally well on the given dataset, correctly classifying all instances.

**Overfitting:** The model could be overfitting the training data, memorizing the patterns and not generalizing well to new, unseen data.

```{r}
conf_matrix_df <- as.data.frame(as.table(conf_matrix))
ggplot(conf_matrix_df, aes(x = Var1, y = predicted_labels, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix Heatmap",
       x = "Predicted Label",
       y = "True Label")

```

## Analysis of Confusion Matrix:

- 2323 instances were correctly predicted as class 0 (true negatives).
- 1323 instances were correctly predicted as class 1 (true positives).
- There were 4 instances where the actual class was 0 but were predicted as class 1 (false positives).
- There were no instances where the actual class was 1 but were predicted as class 0 (false negatives).

```{r}
roc_curve <- roc(test_data$New.Amenity, predicted_probs)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
```


```{r}
head(train_data)
train_data$New.Amenity <- factor(train_data$New.Amenity, levels = c(0, 1))
knn_model <- train(New.Amenity ~ ., data = train_data, method = "knn")
```

```{r}
summary(knn_model)

```

```{r}


# Check levels in test data
levels(test_data$New.Amenity)
```

```{r}
levels(train_data$New.Amenity)
```


```{r}
test_data$New.Amenity <- factor(test_data$New.Amenity, levels = c(0, 1))
predictions <- predict(knn_model, newdata = test_data)
```
```{r}
head(test_data)
```

```{r}
# Confusion Matrix and Accuracy
conf_matrix <- confusionMatrix(predictions, test_data$New.Amenity)
print(conf_matrix)
```

```{r}
length(predictions)
length(test_data$New.Amenity)
```

```{r}
# Data Exploration
summary(accidents_data)

# Data Preprocessing
# Check for missing values
sum(is.na(accidents_data))
accidents_data$Weather_Condition <- as.factor(accidents_data$Weather_Condition)
accidents_data$Wind_Direction <- as.factor(accidents_data$Wind_Direction)
accidents_data$Sunrise_Sunset <- as.factor(accidents_data$Sunrise_Sunset)

# Create a binary variable indicating high or low severity (you can define a threshold)
accidents_data$HighSeverity <- ifelse(accidents_data$Severity > 2, 1, 0)
```

```{r}
set.seed(123)
split_index <- sample(nrow(accidents_data), size = 4000)
train_data <- accidents_data[split_index, ]
test_data <- accidents_data[-split_index, ]

# Select relevant features based on your domain knowledge
selected_features <- c('Temperature.F.', 'Humidity...', 'Pressure.in.', 'Visibility.mi.', 'Wind_Direction')
colnames(train_data)
train_data[,selected_features]

# Train a predictive model (using logistic regression in this case)
model <- glm(HighSeverity ~ ., data = train_data[, c('HighSeverity', selected_features)], family = "binomial")

# Make predictions on the test set
predictions <- predict(model, newdata = test_data[, selected_features], type = "response")

# Convert probabilities to binary predictions
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Convert actual outcomes to factor with the same levels as binary_predictions
test_data$HighSeverity <- as.factor(test_data$HighSeverity)
binary_predictions <- as.factor(binary_predictions)
```

```{r}
# Evaluate the model
confusion_matrix <- confusionMatrix(binary_predictions, test_data$HighSeverity)
print(confusion_matrix)
```

```{r}
# Create a binary variable indicating the presence of an amenity
accidents_data$Amenity_binary <- ifelse(accidents_data$Amenity == 'False', 0, 1)
accidents_data$Amenity_binary <- as.factor(accidents_data$Amenity_binary)
split_index <- sample(nrow(accidents_data), size = 4000)
train_data <- accidents_data[split_index, ]
test_data <- accidents_data[-split_index, ]

# Train a predictive model (using logistic regression in this case)
selected_features <- c('Severity', 'Pressure.in.', 'Visibility.mi.', 'Wind_Direction','Temperature.F.')
model <- glm(Amenity_binary ~ ., data = train_data[, c('Amenity_binary', selected_features)], family = "binomial",maxit = 1000)

# Make predictions on the test set
predictions <- predict(model, newdata = test_data[, selected_features], type = "response")

# Convert probabilities to binary predictions
binary_predictions <- ifelse(predictions > 0.5, 1, 0)
binary_predictions <- as.factor(binary_predictions)

# Evaluate the model
binary_predictions <- as.factor(binary_predictions)
confusion_matrix <- confusionMatrix(binary_predictions, test_data$Amenity_binary)
print(confusion_matrix)
```

```{r}
# Data Preprocessing
# Convert the 'Start_Time' column to a datetime object
accidents_data$Start_Time <- as.POSIXct(accidents_data$Start_Time, format="%Y-%m-%d %H:%M:%S")
accidents_data$DayOfWeek <- weekdays(accidents_data$Start_Time)
accidents_data$HourOfDay <- format(accidents_data$Start_Time, "%H")

# Convert day of the week to a factor
accidents_data$DayOfWeek <- factor(accidents_data$DayOfWeek, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

# Convert hour of the day to numeric
accidents_data$HourOfDay <- as.numeric(accidents_data$HourOfDay)

# Create a binary variable indicating high or low chance of accidents (you can define a threshold)
accidents_data$HighChance <- ifelse(accidents_data$Severity > 2, 1, 0)

# Split the dataset into training and testing sets
set.seed(123)
split_index <- sample(nrow(accidents_data), size = 4000)
train_data <- accidents_data[split_index, ]
test_data <- accidents_data[-split_index, ]

# Train a predictive model (using logistic regression in this case)
model <- glm(HighChance ~ DayOfWeek + HourOfDay, data = train_data, family = "binomial")

# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Evaluate the model
binary_predictions <- as.factor(binary_predictions)
test_data$HighChance <- as.factor(test_data$HighChance)
confusion_matrix <- confusionMatrix(binary_predictions, test_data$HighChance)
print(confusion_matrix)
```

```{r}
model <- glm(Amenity ~ Sevprop_mod + `Response time` , data = dataframe_cleaned, family = binomial)
summary(model)
predicted_probabilities <- predict(model, type = "response")
predicted_outcomes <- ifelse(predicted_probabilities > 0.5, 1, 0)
actual_outcomes <- dataframe_cleaned$Amenity
accuracy <- mean(predicted_outcomes == actual_outcomes)
print(accuracy)
```
```{r}
model <- glm(New_Amenity ~ Weights_mod, data = dataframe_cleaned, family = binomial)
summary(model)
predicted_probabilities <- predict(model, type = "response")
predicted_outcomes <- ifelse(predicted_probabilities > 0.001, 1, 0)
actual_outcomes <- dataframe_cleaned$Amenity
accuracy <- mean(predicted_outcomes == actual_outcomes)
print(accuracy)
```

