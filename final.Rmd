---
title: "US_Accidents_Prediction"
author: "Team: Bhoomika Nanjaraja, Khush Shah, Apoorva Reddy Bagepalli, Devarsh Apurva Sheth"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes some helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
# You will need to install it (once) from GitHub.
# library(devtools)
# devtools::install_github("physicsland/ezids")
# Then load the package in your R session.
library(ezids)
library(ggplot2)
library(caTools)
library(glmnet)
library(caret)
library(pROC)
library(PRROC)
library(dplyr)
library(lubridate)
library(tidyverse)
library(scales)
library(plotly)
library(gridExtra)
library(tidytext)
library(modelr)
library(caret)
library(ROSE)
library(randomForest)
library(ggrepel)
library(patchwork)
library(readr)
library(corrplot)
```

#Introduction
In response to the pressing issue of road safety in the United States, the project "Navigating Road Safety: Insights & Predictions on US Accidents (2016-2023)" has evolved its focus from predicting the severity of accidents to a novel proposal centered on forecasting potential locations for the introduction of new amenities. Acknowledging the staggering annual occurrence of over 6 million passenger car accidents, the project adopts a comprehensive approach, dissecting the intricate interplay of environmental and man-made factors contributing to these incidents. By analyzing years of data, the study aims to decode patterns and predictors that go beyond quantifying incidents, emphasizing the imperative of understanding multifaceted causes. The proposed features include city, street, severity proportion, existing amenities, response time, and a calculated weight based on severity proportion multiplied by response time added to the amenity factor. This innovative model aspires not only to identify potential locations for new amenities but also to pioneer strategies for mitigating risks and enhancing passenger safety, thereby catalyzing transformative changes in road safety measures and reducing casualties. The overarching objective is a testament to the belief that preventing accidents is a significant stride towards preserving human life and improving the quality of road travel for all.\

# Data Description


**Feature Selection**

# SMART Questions

1. Is there a notable contrast in the severity of accidents across various cities and streets?\
2. How does the response time differ among different cities?\
3. In terms of amenities, how does the comparison between existing and new amenities vary across cities?\

# Exploratory Data Analysis

Reading the US Accidents Related data from CSV File.
```{r}
df = read.csv('C:/Users/nupur/computer/Desktop/Intro_DS/DATS6101_Project/US_Accidents.csv')
head(df, 5)
```

```{r}
str(df)
sum(is.na(df))
```

Boxplot of distribution of the Weighted value of Severity Proportion, Response Time and Old Amentiy
```{r}
df$Weighted_Response_Time <- df$Weights

# Alternatively, plot a boxplot to visualize the distribution
ggplot(df, aes(x = 1, y = Weighted_Response_Time)) +
  geom_boxplot(fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Boxplot of Weighted Response Times",
       x = "",
       y = "Weighted Response Time") +
  coord_cartesian(ylim = c(0, 0.0001))

```
The Weighted value are generally fast, with a median of only 7.5e-05 seconds.\

The Weighted value are somewhat variable, with an IQR of 2.5e-05 seconds.\

There are a few outliers, but the overall variability of the Weighted value is relatively low.\

The Weighted value is 9.0e-05 seconds, which is slightly higher than the median. This suggests that the more important value are generally slower than the less important values.\


Highest and Lowest Severity in the given dataset
```{r}
# Load the dataset (assuming your dataset is stored in a variable called 'accidents_data')

# Identify streets with the highest severity proportions
highest_severity_streets <- df %>%
  arrange(desc(SeverityProportion)) %>%
  slice(1)

# Identify streets with the lowest severity proportions
lowest_severity_streets <- df %>%
  arrange(SeverityProportion) %>%
  slice(1)

# Display the results
cat("Streets with the Highest Severity Proportions:\n")
print(highest_severity_streets[, c("City", "Street", "SeverityProportion")])

cat("\nStreets with the Lowest Severity Proportions:\n")
print(lowest_severity_streets[, c("City", "Street", "SeverityProportion")])
```
The table shows that Houston has the highest severity, with a value of 0.000750021. Atlanta has the lowest severity, with a value of 1.75e-07.\


```{r}
dataframe_cleaned <- na.omit(df)
str(dataframe_cleaned)

dataframe_cleaned$City <- as.factor(dataframe_cleaned$City)
names(dataframe_cleaned)[names(dataframe_cleaned) == "New Amenity"] <- "New_Amenity"
dataframe_cleaned$Amenity<- as.factor(dataframe_cleaned$Amenity)
dataframe_cleaned$New_Amenity<- as.factor(dataframe_cleaned$New_Amenity)
#dataframe_cleaned$`Response time`<- as.factor(dataframe_cleaned$`Response time`)
dataframe_cleaned$CityAbr <- substr(dataframe_cleaned$City, start = 1, stop = 3)
```

```{r}
ggplot(dataframe_cleaned, aes(x = CityAbr,fill = New_Amenity)) + geom_bar(position = "dodge")
```

```{r}
ggplot(dataframe_cleaned, aes(x = CityAbr,fill = Amenity)) + geom_bar(position = "dodge")
```

```{r}
category_counts <- table(dataframe_cleaned$Amenity)
barplot(category_counts, 
        main = "Bar Plot of Categories", 
        xlab = "Categories", 
        ylab = "Frequency",
        col = "blue")
```

```{r}
category_counts <- table(dataframe_cleaned$New_Amenity)
barplot(category_counts, 
        main = "Bar Plot of Categories", 
        xlab = "Categories", 
        ylab = "Frequency",
        col = "blue")
```

```{r}
ggplot(dataframe_cleaned, aes(x = `Response time`,fill = Amenity)) + geom_bar(position = "dodge")
```

```{r}
ggplot(dataframe_cleaned, aes(x = `Response time`,fill = New_Amenity)) + geom_bar(position = "dodge")
```

```{r}
dataframe_cleaned$Sevprop_mod <- dataframe_cleaned$SeverityProportion * 1000000
dataframe_cleaned$Weights_mod <- dataframe_cleaned$Weights * 10000
unique_values1 <- unique(dataframe_cleaned$Sevprop_mod)
print(unique_values1)
unique_values2 <- unique(dataframe_cleaned$Weights_mod)
print(unique_values2)
```
```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod)) + 
     geom_boxplot() +
  labs(title = "Boxplot", 
       x = "SeverityProp", 
        y = "Count")+scale_x_continuous(limits = c(1, 10))
```
```{r}
ggplot(dataframe_cleaned, aes(x = Weights_mod)) + 
     geom_boxplot() +
  labs(title = "Boxplot", 
       x = "Weight", 
        y = "Count")+scale_x_continuous(limits = c(1, 10))
```
```{r}
df_sub <- subset(dataframe_cleaned,dataframe_cleaned$Amenity==0)
ggplot(df_sub, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```

```{r}
df_sub <- subset(dataframe_cleaned,dataframe_cleaned$New_Amenity==0)
ggplot(df_sub, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```

```{r}
df_subnew <- subset(dataframe_cleaned,dataframe_cleaned$Amenity==1)
ggplot(df_subnew, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```
```{r}
df_subnew <- subset(dataframe_cleaned,dataframe_cleaned$New_Amenity==1)
ggplot(df_subnew, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of MPG vs. Weight") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```
```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod, y = Weights_mod,color = New_Amenity)) +
    geom_point() +  
    ggtitle("Scatter Plot with Hue") +
    xlab("Sev(Prop)") +
    ylab("Weights") 
```
```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod, y = Weights_mod,color = Amenity)) +
    geom_point() +  
    ggtitle("Scatter Plot with Hue") +
    xlab("Sev(Prop)") +
    ylab("Weights") 
```

```{r}
numeric_data <- dataframe_cleaned[, sapply(dataframe_cleaned, is.numeric)]
cor_matrix <- cor(numeric_data)
corrplot(cor_matrix, method = "number")
```


Modelling
```{r}
set.seed(123)
```

## Train-Test Split Explanation

The provided R code performs a train-test split on a dataframe (`df`). This is a common practice in machine learning to assess the performance of a model on unseen data.
```{r}
indices <- sample(1:nrow(df), size = 0.72 * nrow(df))

# Create train and test datasets
train_data <- df[indices, ]
test_data <- df[-indices, ]
```

```{r}
train_data <- train_data[, !(colnames(train_data) %in% c('City', 'Street'))]
model_formula <- New.Amenity ~ .
model <- glm(model_formula, train_data, family = "binomial")
summary(model)
```
## Logit Equation:

\[
\text{{logit}}(p) = -4.784 \times 10^3 - 4.46 \times 10^8 \times \text{{Severity\_Proportion}} - 5.626 \times 10^8 \times \text{{Amenity}} - 38.94 \times \text{{Response\_time}} + 5.626 \times 10^8 \times \text{{Weights}}
\]


## Interpretation of Logistic Regression Coefficients

- **Intercept (-4.784e+03):** The intercept represents the log-odds of the response variable being 1 when all predictor variables are zero. A large negative value suggests a low probability of the response variable being 1 in the absence of other factors.

- **Severity_Proportion (-4.46e+08):** For every one-unit increase in Severity_Proportion, the log-odds of the response variable being 1 decrease by \(4.46 \times 10^8\). The negative sign suggests that higher Severity_Proportion is associated with a lower probability of the event.

- **Amenity (-5.626e+08):** Similarly, for every one-unit increase in Amenity, the log-odds of the response variable being 1 decrease by \(5.626 \times 10^8\). The negative sign suggests that higher Amenity is associated with a lower probability of the event.

- **Response_time (-38.94):** For every one-unit increase in Response_time, the log-odds of the response variable being 1 decrease by 38.94. The negative sign suggests that a longer Response_time is associated with a lower probability of the event.

- **Weights (5.626e+08):** For every one-unit increase in Weights, the log-odds of the response variable being 1 increase by \(5.626 \times 10^8\). The positive sign suggests that higher Weights are associated with a higher probability of the event.

**Overall Interpretation:**
- The significant coefficients indicate that Severity_Proportion, Amenity, Response_time, and Weights are statistically associated with the response variable.
- Higher values of Severity_Proportion and Amenity are associated with a lower probability of the event.
- Longer Response_time is associated with a lower probability of the event.
- Higher Weights are associated with a higher probability of the event.

```{r}
test_data <- test_data[, !(colnames(test_data) %in% c('City', 'Street'))]
head(test_data,5)
```

```{r}
predicted_probs <- predict(model, newdata = test_data, type = "response")
predicted_labels <- ifelse(predicted_probs > 0.0001, 1, 0)

# Confusion Matrix and Accuracy
conf_matrix <- table(test_data$New.Amenity, predicted_labels)
accuracy <- sum(diag(conf_matrix))/sum(conf_matrix)
print(paste("Accuracy:", accuracy))
```

- **Predicting Probabilities:**
The predict function is used to obtain predicted probabilities from the trained model (model) on the test dataset (test_data).

- **Converting Probabilities to Binary Labels:**
The predicted probabilities are then converted into binary labels using a threshold of 0.0001. If the predicted probability is greater than 0.0001, the label is set to 1; otherwise, it is set to 0.

- **Creating Confusion Matrix:**
A confusion matrix (conf_matrix) is created by comparing the predicted binary labels with the true labels from the test dataset (test_data$New_Amenity).

- **Calculating Accuracy:**
The accuracy of the model is calculated by summing the diagonal elements (correct predictions) of the confusion matrix and dividing it by the total number of predictions.

The accuracy is calculated as the sum of correct predictions (diagonal elements of the confusion matrix) divided by the total number of predictions. If the accuracy is printed as "1," it suggests that all predictions made by the model on the test dataset were correct. This could be due to a few reasons:

**Perfect Model:** The model might be performing exceptionally well on the given dataset, correctly classifying all instances.

**Overfitting:** The model could be overfitting the training data, memorizing the patterns and not generalizing well to new, unseen data.

```{r}
conf_matrix_df <- as.data.frame(as.table(conf_matrix))
ggplot(conf_matrix_df, aes(x = Var1, y = predicted_labels, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix Heatmap",
       x = "Predicted Label",
       y = "True Label")

```

## Analysis of Confusion Matrix:

- 2323 instances were correctly predicted as class 0 (true negatives).
- 1323 instances were correctly predicted as class 1 (true positives).
- There were 4 instances where the actual class was 0 but were predicted as class 1 (false positives).
- There were no instances where the actual class was 1 but were predicted as class 0 (false negatives).

```{r}
roc_curve <- roc(test_data$New.Amenity, predicted_probs)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
```


```{r}
head(train_data)
train_data$New.Amenity <- factor(train_data$New.Amenity, levels = c(0, 1))
knn_model <- train(New.Amenity ~ ., data = train_data, method = "knn")
```

```{r}
summary(knn_model)

```

```{r}


# Check levels in test data
levels(test_data$New.Amenity)
```

```{r}
levels(train_data$New.Amenity)
```


```{r}
test_data$New.Amenity <- factor(test_data$New.Amenity, levels = c(0, 1))
predictions <- predict(knn_model, newdata = test_data)
```
```{r}
head(test_data)
```

```{r}
# Confusion Matrix and Accuracy
conf_matrix <- confusionMatrix(predictions, test_data$New.Amenity)
print(conf_matrix)
```

```{r}
length(predictions)
length(test_data$New.Amenity)
```

```{r}
# Data Exploration
summary(accidents_data)

# Data Preprocessing
# Check for missing values
sum(is.na(accidents_data))
accidents_data$Weather_Condition <- as.factor(accidents_data$Weather_Condition)
accidents_data$Wind_Direction <- as.factor(accidents_data$Wind_Direction)
accidents_data$Sunrise_Sunset <- as.factor(accidents_data$Sunrise_Sunset)

# Create a binary variable indicating high or low severity (you can define a threshold)
accidents_data$HighSeverity <- ifelse(accidents_data$Severity > 2, 1, 0)
```

```{r}
set.seed(123)
split_index <- sample(nrow(accidents_data), size = 4000)
train_data <- accidents_data[split_index, ]
test_data <- accidents_data[-split_index, ]

# Select relevant features based on your domain knowledge
selected_features <- c('Temperature.F.', 'Humidity...', 'Pressure.in.', 'Visibility.mi.', 'Wind_Direction')
colnames(train_data)
train_data[,selected_features]

# Train a predictive model (using logistic regression in this case)
model <- glm(HighSeverity ~ ., data = train_data[, c('HighSeverity', selected_features)], family = "binomial")

# Make predictions on the test set
predictions <- predict(model, newdata = test_data[, selected_features], type = "response")

# Convert probabilities to binary predictions
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Convert actual outcomes to factor with the same levels as binary_predictions
test_data$HighSeverity <- as.factor(test_data$HighSeverity)
binary_predictions <- as.factor(binary_predictions)
```

```{r}
# Evaluate the model
confusion_matrix <- confusionMatrix(binary_predictions, test_data$HighSeverity)
print(confusion_matrix)
```


#Apoorva's
```{r}
# Data Preprocessing
# Check for missing values
sum(is.na(accidents_data))
accidents_data$Weather_Condition <- as.factor(accidents_data$Weather_Condition)
accidents_data$Wind_Direction <- as.factor(accidents_data$Wind_Direction)
accidents_data$Sunrise_Sunset <- as.factor(accidents_data$Sunrise_Sunset)

# Create a binary variable indicating high or low severity (you can define a threshold)
accidents_data$HighSeverity <- ifelse(accidents_data$Severity > 2, 1, 0)
# Split the dataset into training and testing sets
set.seed(123)
split_index <- sample(nrow(accidents_data), size = 4000)
train_data <- accidents_data[split_index, ]
test_data <- accidents_data[-split_index, ]

# Select relevant features based on your domain knowledge
selected_features <- c('Temperature.F.', 'Humidity...', 'Pressure.in.', 'Visibility.mi.', 'Wind_Direction')
colnames(train_data)
train_data[,selected_features]

# Train a predictive model (using logistic regression in this case)
model <- glm(HighSeverity ~ ., data = train_data[, c('HighSeverity', selected_features)], family = "binomial")

# Make predictions on the test set
predictions <- predict(model, newdata = test_data[, selected_features], type = "response")

# Convert probabilities to binary predictions
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Convert actual outcomes to factor with the same levels as binary_predictions
test_data$HighSeverity <- as.factor(test_data$HighSeverity)
binary_predictions <- as.factor(binary_predictions)
# Evaluate the model
confusion_matrix <- confusionMatrix(binary_predictions, test_data$HighSeverity)
print(confusion_matrix)
```
In the data preprocessing stage, missing values were checked and categorical variables were converted into factors. A binary variable, HighSeverity, denoting accidents with severity greater than 2, was created. The dataset was then split into training and testing sets, and relevant features such as temperature, humidity, pressure, visibility, and wind direction were selected based on domain knowledge. A logistic regression model was trained using these features, and predictions on the test set were made, with probabilities converted to binary outcomes using a 0.5 threshold. The model's performance evaluation revealed an accuracy of 81.35%, with a 95% confidence interval between 81.32% and 81.38%. Comparison to the No Information Rate yielded a p-value of 0.5141, indicating no significant difference. The Kappa statistic was minimal at 1e-04, suggesting limited agreement beyond chance. Mcnemar's test, however, indicated a significant difference in performance between predicted and actual outcomes with a highly significant p-value of <2e-16.\


```{r}
model <- glm(Amenity ~ Sevprop_mod + `Response time` , data = dataframe_cleaned, family = binomial)
summary(model)
predicted_probabilities <- predict(model, type = "response")
predicted_outcomes <- ifelse(predicted_probabilities > 0.5, 1, 0)
actual_outcomes <- dataframe_cleaned$Amenity
accuracy <- mean(predicted_outcomes == actual_outcomes)
print(accuracy)
```
```{r}
model <- glm(New_Amenity ~ Weights_mod, data = dataframe_cleaned, family = binomial)
summary(model)
predicted_probabilities <- predict(model, type = "response")
predicted_outcomes <- ifelse(predicted_probabilities > 0.001, 1, 0)
actual_outcomes <- dataframe_cleaned$Amenity
accuracy <- mean(predicted_outcomes == actual_outcomes)
print(accuracy)
```

