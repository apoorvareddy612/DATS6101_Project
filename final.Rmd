---
title: "US_Accidents_Prediction"
author: "Team: Bhoomika Nanjaraja, Khush Shah, Apoorva Reddy Bagepalli, Devarsh Apurva Sheth"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes some helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
# You will need to install it (once) from GitHub.
# library(devtools)
# devtools::install_github("physicsland/ezids")
# Then load the package in your R session.
library(ezids)
library(ggplot2)
library(caTools)
library(glmnet)
library(caret)
library(pROC)
library(PRROC)
library(dplyr)
library(lubridate)
library(tidyverse)
library(scales)
library(plotly)
library(gridExtra)
library(tidytext)
library(modelr)
library(caret)
library(ROSE)
library(randomForest)
library(ggrepel)
library(patchwork)
library(readr)
library(corrplot)
```

#Introduction
In response to the pressing issue of road safety in the United States, the project "Navigating Road Safety: Insights & Predictions on US Accidents (2016-2023)" has evolved its focus from predicting the severity of accidents to a novel proposal centered on forecasting potential locations for the introduction of new amenities. Acknowledging the staggering annual occurrence of over 6 million passenger car accidents, the project adopts a comprehensive approach, dissecting the intricate interplay of environmental and man-made factors contributing to these incidents. By analyzing years of data, the study aims to decode patterns and predictors that go beyond quantifying incidents, emphasizing the imperative of understanding multifaceted causes. The proposed features include city, street, severity proportion, existing amenities, response time, and a calculated weight based on severity proportion multiplied by response time added to the amenity factor. This innovative model aspires not only to identify potential locations for new amenities but also to pioneer strategies for mitigating risks and enhancing passenger safety, thereby catalyzing transformative changes in road safety measures and reducing casualties. The overarching objective is a testament to the belief that preventing accidents is a significant stride towards preserving human life and improving the quality of road travel for all.\

# Data Description


**Feature Selection**

# SMART Questions

1. Is there a notable contrast in the severity of accidents across various cities and streets?\
2. How does the response time differ among different cities?\
3. In terms of amenities, how does the comparison between existing and new amenities vary across cities?\

# Exploratory Data Analysis

Reading the US Accidents Related data from CSV File.
```{r}
df = read.csv('C:/Users/nupur/computer/Desktop/Intro_DS/DATS6101_Project/US_Accidents.csv')
head(df, 5)
```

```{r}
str(df)
sum(is.na(df))
```

Boxplot of distribution of the Weighted value of Severity Proportion, Response Time and Old Amentiy
```{r}
df$Weighted_Response_Time <- df$Weights

# Alternatively, plot a boxplot to visualize the distribution
ggplot(df, aes(x = 1, y = Weighted_Response_Time)) +
  geom_boxplot(fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Boxplot of Weighted Response Times",
       x = "",
       y = "Weighted Response Time") +
  coord_cartesian(ylim = c(0, 0.0001))

```
The Weighted value are generally fast, with a median of only 7.5e-05 seconds.\

The Weighted value are somewhat variable, with an IQR of 2.5e-05 seconds.\

There are a few outliers, but the overall variability of the Weighted value is relatively low.\

The Weighted value is 9.0e-05 seconds, which is slightly higher than the median. This suggests that the more important value are generally slower than the less important values.\


Highest and Lowest Severity in the given dataset
```{r}
# Load the dataset (assuming your dataset is stored in a variable called 'accidents_data')

# Identify streets with the highest severity proportions
highest_severity_streets <- df %>%
  arrange(desc(SeverityProportion)) %>%
  slice(1)

# Identify streets with the lowest severity proportions
lowest_severity_streets <- df %>%
  arrange(SeverityProportion) %>%
  slice(1)

# Display the results
cat("Streets with the Highest Severity Proportions:\n")
print(highest_severity_streets[, c("City", "Street", "SeverityProportion")])

cat("\nStreets with the Lowest Severity Proportions:\n")
print(lowest_severity_streets[, c("City", "Street", "SeverityProportion")])
```
The table shows that Houston has the highest severity, with a value of 0.000750021. Atlanta has the lowest severity, with a value of 1.75e-07.\

Converting the category columns into factor columns
```{r}
dataframe_cleaned <- na.omit(df)
str(dataframe_cleaned)

dataframe_cleaned$City <- as.factor(dataframe_cleaned$City)
names(dataframe_cleaned)[names(dataframe_cleaned) == "New Amenity"] <- "New_Amenity"
dataframe_cleaned$Amenity<- as.factor(dataframe_cleaned$Amenity)
dataframe_cleaned$New_Amenity<- as.factor(dataframe_cleaned$New_Amenity)
#dataframe_cleaned$`Response time`<- as.factor(dataframe_cleaned$`Response time`)
dataframe_cleaned$CityAbr <- substr(dataframe_cleaned$City, start = 1, stop = 3)
```

Distribution of New_Amenity categories across different cities.
```{r}
ggplot(dataframe_cleaned, aes(x = CityAbr,fill = New_Amenity)) + geom_bar(position = "dodge")
```
The provided visualization suggests that the proportion of absent amenities across cities is higher than the proportion of present amenities, but it also shows that there is now a more balanced distribution between the two categories of the target variable.

Distribution of Amenity categories across different cities.
```{r}
ggplot(dataframe_cleaned, aes(x = CityAbr,fill = Amenity)) + geom_bar(position = "dodge")
```
The provided visualization suggests that the proportion of absent amenities across cities is significantly higher than the proportion of present amenities, indicating an imbalance between the two classes of the target variable.

Bar Plot representing the frequency of each unique category of Amenity
```{r}
category_counts <- table(dataframe_cleaned$Amenity)
barplot(category_counts, 
        main = "Bar Plot of Categories", 
        xlab = "Categories", 
        ylab = "Frequency",
        col = "blue")
```
Ratio = Count of "no amenity" / Count of "presence of amenity" = 12,232 / 769 ≈ 15.89
The ratio in this context represents the class imbalance between the two categories of the target variable, where "no amenity" is significantly more common (approximately 15.89 times) than "presence of amenity." This means that the dataset is skewed towards the "no amenity" category, and it's imbalanced in favor of the majority class ("no amenity").

Bar Plot representing the frequency of each unique category of New_Amenity
```{r}
category_counts <- table(dataframe_cleaned$New_Amenity)
barplot(category_counts, 
        main = "Bar Plot of Categories", 
        xlab = "Categories", 
        ylab = "Frequency",
        col = "blue")
```
Ratio = Count of "no amenity" / Count of "presence of amenity" = 8,199 / 4,802 ≈ 1.71
The ratio in this context represents the class imbalance between the two categories of the target variable, where "no amenity" is approximately 1.71 times more common than "presence of amenity." A ratio of 1.71 suggests that there is some imbalance in the dataset, but it's not as severe as in case with the older amenity.

Grouped bar chart where the x-axis represents different Response time values, and for each Response time value, there are bars representing different categories of Amenity
```{r}
ggplot(dataframe_cleaned, aes(x = `Response time`,fill = Amenity)) + geom_bar(position = "dodge")
```
It appears that the Amenity category "0" has a higher frequency across most of the response time intervals compared to the category "1", which has a much lower frequency.The distribution of the response times for Amenity "0" seems to be right-skewed, meaning there are a number of outliers with higher response times. Meanwhile, the distribution for Amenity "1" is less clear due to the lower counts but follows a similar pattern.

Grouped bar chart where the x-axis represents different Response time values, and for each Response time value, there are bars representing different categories of New_Amenity
```{r}
ggplot(dataframe_cleaned, aes(x = `Response time`,fill = New_Amenity)) + geom_bar(position = "dodge")
```
Category "0" still has a higher frequency across most response time bins compared to category "1".
Both categories show a right-skewed distribution, with the majority of the data points falling on the left side of the graph, indicating that lower response times are more frequent for both amenities.
The distribution of the response times for the New_Amenity "0" has some prominent peaks, suggesting that there are specific response time intervals that are particularly common for this category.
The distribution for the New_Amenity "1" appears more even across different response times but with significantly fewer counts than the category "0".

Scaling or normalizing the data in order to make the data more interpretable.
```{r}
dataframe_cleaned$Sevprop_mod <- dataframe_cleaned$SeverityProportion * 1000000
dataframe_cleaned$Weights_mod <- dataframe_cleaned$Weights * 10000
unique_values1 <- unique(dataframe_cleaned$Sevprop_mod)
print(unique_values1)
unique_values2 <- unique(dataframe_cleaned$Weights_mod)
print(unique_values2)
```

Boxplot representing the distribution and summary statistics of the Severity Proportion.
```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod)) + 
     geom_boxplot() +
  labs(title = "Boxplot", 
       x = "SeverityProp", 
        y = "Count")+scale_x_continuous(limits = c(1, 10))
```
In this boxplot, the box is very narrow, indicating that the middle 50% of the data points are very close in value.The data is heavily skewed to the left, with the majority of the data points falling in a very narrow range on the left side of the plot.The presence of multiple outliers on the right suggests that there are instances of unusually high 'SeverityProp' scores.

Boxplot representing the distribution and summary statistics of Weight.
```{r}
ggplot(dataframe_cleaned, aes(x = Weights_mod)) + 
     geom_boxplot() +
  labs(title = "Boxplot", 
       x = "Weight", 
        y = "Count")+scale_x_continuous(limits = c(1, 10))
```
The data is highly concentrated around a central value, with a significant number of outliers on the higher end. This suggests that most of the data points have lower 'Weight' values, with occasional higher values.

Scatter Plot of Severity Proportion vs. Weight" For Older Amenity where there are no Amenities
```{r}
df_sub <- subset(dataframe_cleaned,dataframe_cleaned$Amenity==0)
ggplot(df_sub, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of Severity Proportion vs. Weight For Older Amenity where there are no Amenities") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```
There is a dense clustering of points toward the bottom left of the plot. This suggests that for lower values of "Sev(Prop)", the weighted values is low.As "Severity Proportion" increases, the data points become more spread out, and the weighted values appears to increase for some cities, with a few reaching very high weighted values.The spread of points seems to widen with increasing "Sev(Prop)", indicating more variability in weighted values at higher levels of "Severity Proportion".

Scatter Plot of Severity Proportion vs. Weight For New Amenity where there are no Amenities
```{r}
df_sub <- subset(dataframe_cleaned,dataframe_cleaned$New_Amenity==0)
ggplot(df_sub, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of Severity Proportion vs. Weight For New Amenity where there are no Amenities") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```
The data points are more concentrated toward the lower end of the Severity Proportion scale. As Severity Proportion increases, the data points become more dispersed.There does not appear to be a clear linear trend or pattern in the scatter plot. The relationship between Severity Proportion and Weights is not immediately evident from the visualization.

Scatter Plot of Severity Proportion vs. Weight For Older Amenity where there are Amenities
```{r}
df_subnew <- subset(dataframe_cleaned,dataframe_cleaned$Amenity==1)
ggplot(df_subnew, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +  # Add points
    ggtitle("Scatter Plot of Severity Proportion vs. Weight For Older Amenity where there are Amenities") +
    xlab("Sev(Prop)") + ylab("Weights")
```
There is a concentration of points at the lower end of the Severity Proportion scale, suggesting that many data points have low Severity Proportion values.As Severity Proportion increases, the data points become more sparse, indicating fewer occurrences of higher Severity Proportion values.The plot shows a few points that are spread out at higher values of Severity Proportion, which could be considered as outliers.There is no clear trend or correlation visible in the scatter plot. The data points do not suggest any obvious linear, exponential, or polynomial relationship between Severity Proportion and Weights.

Scatter Plot of Severity Proportion vs. Weight For New Amenity where there are Amenities
```{r}
df_subnew <- subset(dataframe_cleaned,dataframe_cleaned$New_Amenity==1)
ggplot(df_subnew, aes(x = Sevprop_mod, y = Weights_mod)) +
    geom_point(color = "blue") +
    ggtitle("Scatter Plot of Severity Proportion vs. Weight For New Amenity where there are Amenities") +
    xlab("Sev(Prop)") +
    ylab("Weights")
```
The majority of data points are clustered near the origin, indicating a large number of observations with low Severity Proportion and Weights values.As Severity Proportion increases, the frequency of data points decreases, and they are more spread out along the y-axis.There are a few points that appear as potential outliers with high Weights at low Severity Proportion values.There does not appear to be a clear linear relationship or correlation between Severity Proportion and Weights based on the distribution of points. The data points are mostly clustered near the bottom of the plot with a few scattered points at higher Weights values.

```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod, y = Weights_mod,color = New_Amenity)) +
    geom_point() +  
    ggtitle("Scatter Plot with Hue") +
    xlab("Sev(Prop)") +
    ylab("Weights") 
```
There is a significant clustering of blue points at the lower end of both axes, suggesting that for most observations, both Severity Proportion and Weights have low values.A few blue points are distributed sporadically at higher Weights values, indicating some variability for the "1" category of "New_Amenity"(i.e Indicating the data points with higher weights require the presence of amenities).The red points are few and are located at the lowest Severity Proportion and Weights values, suggesting that the "0" category of "New_Amenity" is associated with low Severity Proportion values.

```{r}
ggplot(dataframe_cleaned, aes(x = Sevprop_mod, y = Weights_mod,color = Amenity)) +
    geom_point() +  
    ggtitle("Scatter Plot with Hue") +
    xlab("Sev(Prop)") +
    ylab("Weights") 
```
There is a notable concentration of red points along the baseline of the plot, indicating a high number of observations with the lowest Weights value across all Severity Proportion values(Indicating absence of amenities for most of the data points).Blue points occur less frequently and are mostly found at lower Severity Proportion values, except for a few instances where they appear at higher "Weights"(Indicating the data points with higher weights require the presence of amenities.)


Modelling
```{r}
set.seed(123)
```

## Train-Test Split Explanation

The provided R code performs a train-test split on a dataframe (`df`). This is a common practice in machine learning to assess the performance of a model on unseen data.
```{r}
indices <- sample(1:nrow(df), size = 0.72 * nrow(df))

# Create train and test datasets
train_data <- df[indices, ]
test_data <- df[-indices, ]
```

```{r}
train_data <- train_data[, !(colnames(train_data) %in% c('City', 'Street'))]
model_formula <- New.Amenity ~ .
model <- glm(model_formula, train_data, family = "binomial")
summary(model)
```
## Logit Equation:

\[
\text{{logit}}(p) = -4.784 \times 10^3 - 4.46 \times 10^8 \times \text{{Severity\_Proportion}} - 5.626 \times 10^8 \times \text{{Amenity}} - 38.94 \times \text{{Response\_time}} + 5.626 \times 10^8 \times \text{{Weights}}
\]


## Interpretation of Logistic Regression Coefficients

- **Intercept (-4.784e+03):** The intercept represents the log-odds of the response variable being 1 when all predictor variables are zero. A large negative value suggests a low probability of the response variable being 1 in the absence of other factors.

- **Severity_Proportion (-4.46e+08):** For every one-unit increase in Severity_Proportion, the log-odds of the response variable being 1 decrease by \(4.46 \times 10^8\). The negative sign suggests that higher Severity_Proportion is associated with a lower probability of the event.

- **Amenity (-5.626e+08):** Similarly, for every one-unit increase in Amenity, the log-odds of the response variable being 1 decrease by \(5.626 \times 10^8\). The negative sign suggests that higher Amenity is associated with a lower probability of the event.

- **Response_time (-38.94):** For every one-unit increase in Response_time, the log-odds of the response variable being 1 decrease by 38.94. The negative sign suggests that a longer Response_time is associated with a lower probability of the event.

- **Weights (5.626e+08):** For every one-unit increase in Weights, the log-odds of the response variable being 1 increase by \(5.626 \times 10^8\). The positive sign suggests that higher Weights are associated with a higher probability of the event.

**Overall Interpretation:**
- The significant coefficients indicate that Severity_Proportion, Amenity, Response_time, and Weights are statistically associated with the response variable.
- Higher values of Severity_Proportion and Amenity are associated with a lower probability of the event.
- Longer Response_time is associated with a lower probability of the event.
- Higher Weights are associated with a higher probability of the event.

```{r}
test_data <- test_data[, !(colnames(test_data) %in% c('City', 'Street'))]
head(test_data,5)
```

```{r}
predicted_probs <- predict(model, newdata = test_data, type = "response")
predicted_labels <- ifelse(predicted_probs > 0.0001, 1, 0)

# Confusion Matrix and Accuracy
conf_matrix <- table(test_data$New.Amenity, predicted_labels)
accuracy <- sum(diag(conf_matrix))/sum(conf_matrix)
print(paste("Accuracy:", accuracy))
```

- **Predicting Probabilities:**
The predict function is used to obtain predicted probabilities from the trained model (model) on the test dataset (test_data).

- **Converting Probabilities to Binary Labels:**
The predicted probabilities are then converted into binary labels using a threshold of 0.0001. If the predicted probability is greater than 0.0001, the label is set to 1; otherwise, it is set to 0.

- **Creating Confusion Matrix:**
A confusion matrix (conf_matrix) is created by comparing the predicted binary labels with the true labels from the test dataset (test_data$New_Amenity).

- **Calculating Accuracy:**
The accuracy of the model is calculated by summing the diagonal elements (correct predictions) of the confusion matrix and dividing it by the total number of predictions.

The accuracy is calculated as the sum of correct predictions (diagonal elements of the confusion matrix) divided by the total number of predictions. If the accuracy is printed as "1," it suggests that all predictions made by the model on the test dataset were correct. This could be due to a few reasons:

**Perfect Model:** The model might be performing exceptionally well on the given dataset, correctly classifying all instances.

**Overfitting:** The model could be overfitting the training data, memorizing the patterns and not generalizing well to new, unseen data.

```{r}
conf_matrix_df <- as.data.frame(as.table(conf_matrix))
ggplot(conf_matrix_df, aes(x = Var1, y = predicted_labels, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix Heatmap",
       x = "Predicted Label",
       y = "True Label")

```

## Analysis of Confusion Matrix:

- 2323 instances were correctly predicted as class 0 (true negatives).
- 1323 instances were correctly predicted as class 1 (true positives).
- There were 4 instances where the actual class was 0 but were predicted as class 1 (false positives).
- There were no instances where the actual class was 1 but were predicted as class 0 (false negatives).

```{r}
roc_curve <- roc(test_data$New.Amenity, predicted_probs)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
```


```{r}
head(train_data)
train_data$New.Amenity <- factor(train_data$New.Amenity, levels = c(0, 1))
knn_model <- train(New.Amenity ~ ., data = train_data, method = "knn")
```

```{r}
summary(knn_model)

```

```{r}


# Check levels in test data
levels(test_data$New.Amenity)
```

```{r}
levels(train_data$New.Amenity)
```


```{r}
test_data$New.Amenity <- factor(test_data$New.Amenity, levels = c(0, 1))
predictions <- predict(knn_model, newdata = test_data)
```
```{r}
head(test_data)
```

```{r}
# Confusion Matrix and Accuracy
conf_matrix <- confusionMatrix(predictions, test_data$New.Amenity)
print(conf_matrix)
```

```{r}
length(predictions)
length(test_data$New.Amenity)
```



```{r}
accidents_data <- read.csv('/Users/apoorvareddy/Downloads/Academic/DATS6101/Project/US_accidents_EDA.csv')

# Data Exploration
summary(accidents_data)
# Data Preprocessing
# Check for missing values
sum(is.na(accidents_data))
accidents_data$Weather_Condition <- as.factor(accidents_data$Weather_Condition)
accidents_data$Wind_Direction <- as.factor(accidents_data$Wind_Direction)
accidents_data$Sunrise_Sunset <- as.factor(accidents_data$Sunrise_Sunset)

# Create a binary variable indicating high or low severity (you can define a threshold)
accidents_data$HighSeverity <- ifelse(accidents_data$Severity > 2, 1, 0)
# Split the dataset into training and testing sets
set.seed(123)
split_index <- sample(nrow(accidents_data), size = 4000)
train_data <- accidents_data[split_index, ]
test_data <- accidents_data[-split_index, ]

# Select relevant features based on your domain knowledge
selected_features <- c('Temperature.F.', 'Humidity...', 'Pressure.in.', 'Visibility.mi.', 'Wind_Direction')
colnames(train_data)
train_data[,selected_features]

# Train a predictive model (using logistic regression in this case)
model <- glm(HighSeverity ~ ., data = train_data[, c('HighSeverity', selected_features)], family = "binomial")

# Make predictions on the test set
predictions <- predict(model, newdata = test_data[, selected_features], type = "response")

# Convert probabilities to binary predictions
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Convert actual outcomes to factor with the same levels as binary_predictions
test_data$HighSeverity <- as.factor(test_data$HighSeverity)
binary_predictions <- as.factor(binary_predictions)
# Evaluate the model
confusion_matrix <- confusionMatrix(binary_predictions, test_data$HighSeverity)
print(confusion_matrix)
```
In the data preprocessing stage, missing values were checked and categorical variables were converted into factors. A binary variable, HighSeverity, denoting accidents with severity greater than 2, was created. The dataset was then split into training and testing sets, and relevant features such as temperature, humidity, pressure, visibility, and wind direction were selected based on domain knowledge. A logistic regression model was trained using these features, and predictions on the test set were made, with probabilities converted to binary outcomes using a 0.5 threshold. The model's performance evaluation revealed an accuracy of 81.35%, with a 95% confidence interval between 81.32% and 81.38%. Comparison to the No Information Rate yielded a p-value of 0.5141, indicating no significant difference. The Kappa statistic was minimal at 1e-04, suggesting limited agreement beyond chance. Mcnemar's test, however, indicated a significant difference in performance between predicted and actual outcomes with a highly significant p-value of <2e-16.\


